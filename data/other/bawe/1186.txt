Moore's law suggests that by 2050 we will have computers as powerful as human brains 1. A few weeks back a discovery was made that could lead to a quantum computer capable of carrying out a staggering 10 30 simultaneous calculations. The Blue Brain project team plans to be doing cellular level whole brain simulations within 20 years. This essay is about the Grand Challenge in Computing 5 - a long term goal for set for research within computing in Britain. It aims to give us a better idea of the relationship between brain and mind by developing a machine with many of the same capabilities as a human. Its primary target for the next 15-20 years is to:
A research team at the Neuroscience Institute already claim to have developed a robot with some of the intelligence of an 18 month old baby 3. They are just one of many research teams working on problems relating to GC5. They work from many different angles but they can broadly be split into two groups, Top-Down - being those teams looking at simulating and finding out more about existing brains, and Bottom-Up, being those which are working on intelligence from first principles upwards. While the actual challenge documentation considers a third group bringing them together I will be looking solely at the two edge ones in this essay.
As relatively little is actually know about the precise structure and functioning of the brain, lots of work is going into simulating them, to save the difficultly of working with real brains. When looking at real brains there are many issues at stake: moral issues, time issues, and practicality issues. There are situations in which an fMRi scanner cannot monitor experiments which cannot morally or legally be performed etc. Thus computational modeling has come in to help out with this.
Bottom-Up research is done on a different set of principles to Top-Down. This time the researchers start with an idea of something that they wish to do and produce a robot to do it based on the principles of systems and mathematics etc. These tend to be small, single purpose devices such as iRobots Roomba, a small, single purpose robot fitted with wall sensors, depth sensors and visual systems - all there with the soul purpose of cleaning people's floor 6. It has a sister robot, called Scooba, which is very similar but this one mops floors instead of vacuuming.
From these few projects I have mentioned here you can get a view on the state of the art in the field. It is a very limited view as I deliberately left out information on whole swathes of research, such as visual systems which tend to blur the boundaries between top down and bottom-up. The point I was trying to get over was that much of the research is steaming on ahead - in completely different directions. And this I believe highlights a fundamental flaw in the nature of this Grand Challenge. While on one hand it calls for research into brain/mind and how intelligence arises, it on the other hand calls for the development of household robotics as its target. While it is a reasonable assumption to say that research into robotics and artificial intelligence is sure to bolster our knowledge on cognitive neuroscience and visa versa, it is not a good plan to link them together into one combined project.
The primary reason for this dubiousness with which I treat this merging of these two areas of research is the loss of scientific freedom caused by it - even if this loss is completely unintentional. One way this could come about would be, for instance, if grants for research in the field were only being granted to ambitious projects if they could relate themselves to a Grand Challenge, what would happen then? As far as I can see, projects looking into areas such as Swarm Robotics would be crushed, as while they have already proved themselves in nature, they are not based on the same model of intelligence as humans, and thus of little actual use to the Grand Challenge in its current form.
For an example of this close minded human-centrism already infiltrating the project let us look again at its 15/20 year target. It wishes to develop a robot capable of doing many different tasks, conversing with the owner, understanding why it is doing things etc. Consider an intelligent house based on a central server, an extensive sensor network around the area comprising of audio, visual, infrared etc. It would have many ways of outputting information - speakers, alarms, visual display units, direct control of appliances etc. It would have direct control over swarms of small robots, capable of doing all the cleaning, gardening, washing etc. The owner would be able to ask the house to do anything, from any area of the house; the house would be able to predict that the owner wanted something done and act on it instantly. It would be able to permanently monitor the state of the house, and make adjustments based on this information. It would not however fit within the wording of the 15/20 year target for this grand challenge.
Now let us compare this idea to that more directly specified by the target - that of a single robot capable of performing all these things. It would only be able to monitor the state of the room it was in, it would be very expensive as it would have to be capable of doing many different tasks using the same body. It would get in the way, as it would have to be on the large side to just perform most tasks required in a human home - and that's not even considering the size it would have to be to contain a powerful enough computer, enough sensors to get relevant data and all the control mechanisms. Importantly it would take time to maneuver around the house and you would have to be in the same vicinity of it to give it commands.
Now I do not mean to knock this idea to heavily. A robot of this type would certainly have advantages but the important point here is that the challenge had imposed an unnecessary restriction on the development of robotics. While this is understandable in the context of the challenge it makes no sense in any other context. To develop a robot in a less than efficient way is a ridiculous use of resources - and realistically would never get the funding in the first place.
Then we come too actually considering the realism of being able to develop a robot from top down principles capable of doing all this, within 20 years. Let us consider the BBD's of Krichmar and his team - he has claimed that they nowadays have the intellectual powers of an 18 th month old baby. A human baby of this age should be beginning to develop language and a theory of mind and possession. Their robots on the other hand are merely capable of such primal things as navigation and classical conditioning. Compare this to a Bee - which when captured, can be taken by car to a location 6 miles away and find its way back, this is about the ability of the Darwin's.
So from this stage they are expected to be able to learn to use tools, and act out of pure altruism for some human "master" - all within 20 years. I cannot help but feel that this is completely unrealistic even if just for the last reason. To be of any real use to Neuroscience, this altruism would need to be a purely emergent behaviour from an ANN similar in science and architecture to a mammalian brain. While this is almost certainly possible, it is something completely unlike anything developed in any higher life form. In fact, the only examples in biology known to act so altruistically are social insects - leading us back to Swarm Intelligence and away from the top-down approach to robotics. So the chances of us being able to do anything useful with top-down research in robotics in the foreseeable future are very low.
This is the second reason why I propose that the Grand Challenge is flawed, and it leads directly from the flaws set down in the first. It is simply unrealistic to expect research to progress in such a way that they are mutually beneficial to both sides of the project in the foreseeable future. As criticism alone is worthless lest it is constructive, I shall not end this argument here. I shall instead propose two new Grand Challenges to take the place of this existing one - one doing computational modeling of brains and the models of related systems for purely neuroscience reasons, the other working towards robotics which would be useful in the home for instance. Using this method there could be set reasonable and attainable goals and a realistic roadmap for the challenges progression could be laid down, and there would be no pretense of them assisting each other.
Some people may say that the first challenge I propose, being purely for the purpose of Neuroscience is not really a challenge in computing. But I deny this forthright. When we look at Neuroscience, what is to be seen but the greatest reverse engineering project ever to be undertaken? The brain was natures answer to computers, and thus those who are trying to work out how it works are but Computer Scientists of a different persuasion. The subject of Biology covers everything from the smallest bacterium through to Gaia in her entirety. Maybe it is time that the two sides of computing united towards a common purpose, without crossed-purposes muddling the relationship.