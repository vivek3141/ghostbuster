In December 1947, the transistor was invented at the Bell Laboratories, sparking the beginning of the digital age. Until the 1970s the computer world was represented by large expensive mainframes that could perform only the most simple of calculations. During the 1970s the idea of a personal computer millions of times more powerful that was small and cheap enough for an individual to own became realistic, and by the late 1980s it had become a huge industry. The field of computer architecture accelerated towards the end of the century at an unprecedented rate. Every year for three decades, new techniques and advances had made the last year's technology obsolete. Computers are now so powerful and small to be ubiquitous, and still they are constantly becoming faster and smaller. But what has been causing this remarkable development, and will it continue? These are issues that will be discussed in this essay.
To do this it is essential to look at the microprocessor. On November 15 th, 1971, Intel released the world's first single chip microprocessor, the 4004. It was 12mm 2 in area, had just over 2,300 transistors and had a speed of 400 KHz. With its 4-bit CPU, command register, decoder, decoding control, control monitoring of machine commands and interim register, the 4004 was a revolutionary piece. It even provided the computational power of the Pioneer 10 spacecraft.
By the following year, Intel had doubled the speed to 800 KHz (3,500 transistors, 10 microns). By 1979 the speed had been multiplied by 10, giving 8MHz (29,000 transistors, 3 microns). By 1990 the speeds were 25 MHz (1,200,000 transistors, 1 micron) and by 2000 it had reached 1.5 GHz (42,000,000 transistors, 0.13 microns).
By analysing this data it is clear that the number of transistors increase by about 55% per year (Moore's law states that the number of transistors per chip doubles every 18 months). This is determined by a number of factors. The feature size of a transistor, as we have seen, has decreased from 10 microns in 1971 to 0.13 microns in 2000. As feature size is a linear measurement, it follows that a linear decrease results in a quadratic increase in transistor density. A quick calculation can confirm that transistor density increases at about 35% per year, and so quadruples in just over 4 years. Die size increases by between 10 and 20% every year.
Feature size does not just reduce on a horizontal plane, but also on a vertical one. This shrinkage requires a lower operating voltage and as a result it is fair to say that transistor performance increases linearly with a linearly decreasing feature size. One issue that is currently rearing its head is that as transistors are getting smaller the interconnecting wires are causing a significant delay time in circuits (Hennessy and Patterson 2003).
It has been predicted by the Semiconductor Industries Association that by 2014 clock speed will reach 16.9 GHz with 3,620,000,000 transistors (, 1998).
Let us turn to memory. In late 1995, SDRAM was introduced with a bandwidth of 50mtps. This increased over the next five years until it reached 133mtps in 2000, when it was replaced with DDR RAM. By late 2003 the bandwidth of DDR RAM had increased to 400mtps, when it was replaced by DDR2 RAM, which currently has a bandwidth o 533mtps. Crucial Technologies (a division of Micron) predicted in late 2004 that by 2007 DDR2 would have reached 800mtps (, 2004).
It can be argued that the improvements in memory have been small in comparison to microprocessors (increasing in performance at less than 10% per year as opposed to the 55% of the microprocessor). The bandwidth of memory increases at about double the rate of the drop in latency. One of the arguments of the DDR RAM over the DDR2 is that the latency is relatively low, so the actual improvements are less still than the statistics indicate (, 2004).
The future of memory is looking uncertain. Japan has traditionally dominated the computer memory chip market. Its multinationals (Fujitsu, Hitachi, NEC, Mitsubishi, Sharp and Toshiba) have controlled the production and supply of memory chips. The main focus of Japan's multinationals, however, has been on entire computers, peripherals, telecommunications equipment and consumer electronic systems. The majority of the memory chips produced by these companies end up in the electronic systems they produce. They have then made a profit by selling the excess production to the world market.
As competitors from Korea and Taiwan have increased market shares, the price of computer memory has gyrated. It has therefore been less expensive for Japan's electronics companies to produce computer memory chips from outside vendors (, 2004).
To look any distance into the future, however, it is necessary to zoom out away from the detail and view the wider picture. History has shown that the most reliable predictions usually come from physicists. Currently they are predicting a cap on the current rise in computer performance in 2020. At this point, the limitations of silicon come into play, and the laws of quantum mechanics will take over from the macro world. If computers are to still increase in performance, these limitations will bring about the necessity for new types of computers such as optical, molecular, quantum and DNA.
It is an accepted fact that in general it takes 15 years for a good new idea to go from being a concept to being widely marketed. An example of this is the personal computer, which started to be very popular in the marketplace about 15 years after the first single chip microprocessor. As quantum computers were first considered in around 1997, this implies that they could be common by 2012 (Kaku, 1998).
In conclusion it can be said that the rate of increase in microprocessor performance has stayed reasonably constant at 55% over the last three decades, and will probably continue to do so until huge changes will be needed in around another 6 years, beyond which it is difficult to predict. Memory has increased in performance at a much lower rate and if problems in latency can be addressed should continue to increase at about the current rate for the foreseeable future.
a)
b)
Replacement of cache misses is primarily controlled by hardware, whereas the operating system primarily controls virtual memory replacement. The size of the processor address determines the size of the virtual memory, whereas cache size is independent of processor address size.
There are three different systems of cache: fully associative, direct mapping or set associative.
In fully associative cache, a block can be placed anywhere and is indexed by content. In direct mapping cache, a block is found by an index (calculated by taking the remainder when dividing the block address by the number of blocks in the cache). The cache entries are then tagged with the upper part of a memory address. Set associative cache is similar, but differs in the fact that a block is found using a set index.
The miss penalty for virtual memory is high, and so developers usually choose lower miss rates over simpler algorithms. Thus operating systems allow blocks to be placed anywhere in main memory.
Both paging and segmentation rely on a data structure that is indexed by the page or segment number, containing the physical address of the block. For segmentation, the offset is added to the segment's physical address. For paging, the offset is concatenated to this physical page address.
When a cache miss occurs, the cache controller must select a block to be discarded. For direct mapping there is no choice as only one block is checked for a hit. For set associative, you can either randomly select a block (easiest to build into hardware) or the least recently used (LRU) block is chosen for replacement. This may be implemented by associating a counter with a block. The counter is incremented at regular intervals or every time the cache is accessed. The counter is reset to zero if the block is accessed. The counter with the highest value indicates the LRU block.
With virtual memory, as the overriding operating systems guideline is minimising page faults, almost all operating systems try to replace the LRU block because it is least likely to be needed.
In cache, it is not possible to modify a block until its tag is checked to see if the address is a hit, thus nothing can be written in parallel with tag checking. It takes longer to write than to read and it may be necessary to write to a specific location within a block.
You can use write-through where information is written to both blocks in cache and in main memory or write-back where information is written to just a block in the cache. The modified cache block is written to main memory only when it is replaced. To reduce frequency of write-backs on replacement a dirty bit is used as a status. The status indicates a block is modified (dirty) or clean. If the block is dirty, it should be written back to main memory on a miss.
With write-back, writes occur at the speed of the cache memory, multiple writes to a block in cache only require one write to main memory and uses less memory bandwidth.
It is easier to implement write-through, on the other hand, and main memory has a copy of the data (which is useful for operating systems and I/O). Another advantage is that the processor waits for writes to complete during write-through. A common optimisation to reduce write stalls is write buffer, which allows the processor to continue as soon as the data is written to the buffer.
As it takes a long time to access the level below main memory, the write strategy of virtual memory is always write-back.
Since the cost of unnecessary access to the next-lower level is so high, virtual memory systems usually include a dirty bit. It allows blocks to be written to disk only if they have been altered since being read from the disk.
To summarise, blocks can be placed in one of three ways in cache, but in only the first of these in virtual memory. Blocks in main memory are either indexed by content or by an index of some description. Blocks in virtual memory are accessed via a data structure.
In cache blocks are either selected randomly after a miss, which is easy but can make another miss, or the LRU block is selected. In virtual memory the LRU block is always selected.
In cache you can either use write-through or write-back, whereas in virtual memory write-back must be used.
In general there are more options open to the developer when designing cache usage, whereas the penalty of a miss makes virtual memory design more restrictive.