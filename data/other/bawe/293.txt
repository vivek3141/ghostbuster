Strayer and Johnston's article; Driven to Distraction, looked at the extent to which mobile phone conversations could interfere with driving. The study concentrates on examining the effects of increased demands on attention during a simulated-driving task.
The research was based on several pieces of background information, including; the links between driver inattention & human error and motor accidents, the increased risk of getting into an accident when using a mobile phone and finally evidence that shows that working memory tasks, mental arithmetic tasks and reasoning tasks can disrupt simulated-driving performance.
The research used two main experiments and an additional control experiment to look at the effects of diverted attention on a simulated-driving task. The first experiment looked at the differences between handheld and handsfree phone conversations on the simulated-driving task by examining reactions to a stimulus change. There were two conditions in this experiment, one where the participant must engage in a conversation with a confederate and the second where the participant listens to a radio show. The second experiment looked more closely at the source of the interference on the simulated-driving task by assessing participants' deviation from an ideal tracking position on a course. It used an easy course and a hard course in both a shadowing task condition and a word-generation task condition. The additional control condition was an extra experiment the authors carried out to look at the effect of forced attention to a speech on the simulated-driving task. The additional control experiment was essentially the same as the first experiment; however participants were also given a multiple-choice questionnaire as a post-test to determine how closely the participant had attended to the material.
The research provided three main findings; that participants missed twice as many signals and took longer to respond to signals when engaged in a phone call, that there were no significant differences in results gained when using either a hands-free phone or a handheld phone, and that errors increased when doing a word-generation task but not when doing a shadowing task. The main conclusion of the research was that conversing on a mobile phone lead to "significant decrements" in simulated-driving performance.
My main concern with this research surrounds the authors' broad generalisations of their findings: that the use of mobile phones increases risks while driving due to the impairment of reactions whilst a conversation occurs. To be able to make narrow-to-broad generalisations aspects of the experiment need to be strong enough to allow inferences to be made, e.g. the setting of the experiment and the similarity between the test and the generalised application. I believe that some of these aspects within this research are not concrete enough to allow generalisation.
I feel that the driving-simulation task does not accurately represent the act of driving; following a marker moving across the screen bears little resemblance to a driver following the road, and the reaction task the participant has to carryout (pressing the button when the tracking stimulus changes to red) does not accurately represent the task of breaking in the case of traffic lights.
This driving-simulation task is used throughout the whole project and the conclusions of the article are formed on the basis of results gained in this task. I believe that there are serious problems with the external validity of the project due to the variations between the experimental task and the activity of driving. A more accurate way to run this test would be to use a more realistic driving simulation such as those used on driving theory tests. A higher level of realism in the test would allow for a more accurate comparison between the experiment and a real-life situation, therefore decreasing the setting variation and increasing both the construct and external validity of the experiments. However, using such a method would mean that the design of the experiments would need to be changed to control individual differences confounds such as driving experience.
The authors do not appear to have support for the two main assumptions that they used in the project, these initial assumptions are used to make generalisations from the experimental results to effects found in real-life. Without suitable support for these assumptions, either through convergent or divergent manipulation checks there is a question as to whether these assumptions are valid. If they are not then this creates large implications for the construct validity of the project.
Another issue with the external validity of all the experiments is that despite all the participants being undergraduate students from the University of Utah, the results are generalised to the population as a whole. This is a sampling bias and can lead to incorrect assumptions of the behaviour of the general population. To allow the authors to make such broad generalisations, a sample from a broader population should have been used. Another issue with the sample is that all the experiments use a very small sample size, leading to problems with the statistical reliability of the whole project. The size of these samples should be increased to increase reliability of the results.
The design of the two main experiments show potential problems with the internal validity of the tests. The time taken for the tests was a relatively long time; the total length of the test for each person was 37 minutes (excluding time taken for giving instructions). This could lead to noise within the results due to changes that naturally occur within individuals over time such as tiredness, boredom and a decrease in motivation. Whilst this was controlled to some extent through placing the single-task both before and after the dual-task it could still lead to noise in the results. The results gained in the first experiment for the reaction times in the radio condition also showed signs of being affected by a practice confound as participants were repeatedly exposed to the same task. A solution to both of these problems would be to decrease the length of the tests.
Within the additional control experiment a multiple-choice questionnaire was given however this design can encourage guesswork which could have been a concurrent factor that affected the results. A solution would be to use a free-response design questionnaire. A further concern with the internal validity lies within the additional control condition experiment in that the passage read to the participants appears to have come from a book rather than a statement created by the authors. This results in a risk that the participants may be familiar with the passage, leading to confounds within the results.
The authors have managed to control for many problems that could have occurred with validity and reliability. Within the experiments external support is provided for the research providing increased external and concrete validity to the project. Within the first experiment the authors provided a convergent manipulation check through results found in another paper, a divergent manipulation check was given by the control experiment for the first experiment. The control test also prevented questions being asked about the construct validity of the first experiment. Without the support of the additional condition it could have been argued that participants did not actually pay attention in the radio task explaining why there were no differences in the results between the single-task and the dual-task conditions, and thus demonstrating that the task was not a match for the constructs being tested.
To ensure that the reliability of experiments is not threatened by imprecise measurements, exact measurements in the smallest measurement appropriate should be taken. In these experiments milliseconds were a very reliable measurement to use to time reactions. However, the report does not state how the 'tracking error' in the second experiment was measured. It is assumed that it is a precise measurement and therefore is not a threat to the external validity and reliability.
Part of the first experiment was a signal detection test where participants indicated, by pressing a button, if they saw the stimulus change to red. Whilst the results for this experiment discuss the miss rate, they do not show the correct rejection rate. This is important as this would highlight any bias within the test; it also allows discrimination to be examined to see if there is any problems with the detection of the signal. These points could highlight problem areas within both the validity and reliability of the experiments.
The experiments used random assignment in a non-equivalent control group design. This minimizes individual difference confounds (e.g. generally slow reaction times). The two main experiments used an incomplete design of counterbalancing, ensuring again that individual difference confounds are minimized. This also ensures that there are no practice effects or confounds caused by the different groups within conditions (i.e. conversation topic in the first experiment). A confederate is used in the first experiment who is blind to the condition the participant is in (i.e. Handsfree or Handheld) therefore reducing the potential for experimenter effects and error variance.
Overall I believe that this piece of research has been carried out thoroughly and most of the problem areas that I have identified are not major ones. My concern is over the concrete validity of the findings as I do not believe that the tests that have been carried out actually provide an answer to the aim of the project. The original aim was to "determine the extent to which cell-phone conversations may interfere with driving" yet the project can only truly conclude that 'conversing on either a handheld or handsfree cell-phone led to significant decrements in simulated-driving performance'. Therefore without the assumptions that the authors make (as examined earlier) no answer to the original aim can be made and, with relation to the aim, the research becomes invalid.